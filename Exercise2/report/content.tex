\section*{Exercise 1}
\label{sec:exercise1}

\subsection*{Part A}

Using the minimax algorithm, we assume that both players play optimally (namely the MAX player maximizes his utility value, while the MIN player minimizes the same value). Furthermore, the algorithm performs a complete depth-first exploration of the corresponding game tree (Figure \ref{fig:exercise1}), assigning the minimum or maximum utility value out of all its children nodes as a minimax value to every MIN or MAX node respectively. 

In this case, MIN nodes B, C, D and E would be given the values -12, -7, -9, 2 respectively, while the starting MAX node will take the maximum of these four values, namely 2. Therefore, the resulting optimal move for the MAX player would lead to E, and the optimal response by the MIN player would then lead to the terminal node with utility value 2.

\subsection*{Part B}

Figure \ref{fig:exercise1} depicts the game tree.

\input{images/exercise1}

\section*{Exercise 4}

As explained in Exercise 1, the minimax algorithm guarantees the optimal solution for the MAX player given that both players make optimal decisions. Assuming that the MIN player chooses a suboptimal action instead, then the value of the respective MIN node (and thus the resulting minimax value of its parental MAX node, and inductively for the whole game tree) cannot decrease because the optimal action already led to the minimum value.

For example, in Figure \ref{fig:exercise1} let us assume that the MIN player always selects the second best available action. These suboptimal moves would then mean that the MAX player would receive 4, 0, -8 or 3 if he opted for B, C, D or E respectively. Since the optimal move found by minimax leads to E, the resulting value would increase from 2 to 3. However, moving to B would actually result in an even better value (4), thus illustrating that when the MAX player can predict a suboptimal play by MIN, there may be better strategies than following the minimax decision.

%\section*{Exercise 1}
%
%\subsection*{Part A}
%
%PEAS descriptions for a robot basketball player, industrial orange-apple sorter and a stock investor are given in Table \ref{table:peas}.
%
%\input{tables/peas}
%
%
%\subsection*{Part B}
%
%Table \ref{table:environments} contains the environmental characteristics of the three aforementioned agent types. In the following paragraphs I select a suitable agent design, providing a brief explanation for each agent type.
%
%
%\input{tables/environments}
%
%\textbf{Robot basketball player:} \textit{Simple reflex agents} cannot perform well in this partially observable, dynamic environment. An \textit{internal-state model} could be used, but we would need many states that are difficult to define in such a continuous-state environment. In addition, simply storing the current state of the system would not be enough for the player to decide its next course of action. Furthermore, a \textit{goal-based agent} would be inefficient since scoring is not the only goal in basketball. Therefore, \textit{a utility-based model} is deemed necessary to provide a more efficient performance measure in this multiagent environment. Online \textit{learning} could also be applied during the match, allowing adaptation against the opposing team. Its application however would be difficult without sacrificing (at least their initial) performance.
%
%\textbf{Industrial orange-apple sorter:} An \textit{internal-state model} would be enough for fruit classification based on the partially observable fruits and stochastic background (e.g. varying light in the room or existing branches/leaves). \textit{Learning} would be useful if we wanted our agent to classify more types of fruits in the future.
%
%\textbf{Stock investor:} Similar to a robot basketball player, this agent needs to consider not only its basic goal of maximizing its net profit (namely buying cheaply and selling expensively), but also other features such as credibility, financial forecasts and cooperation with other investors. Therefore, a \textit{utility-based agent} would again be more appropriate. \textit{Learning} could also be proven useful in adaptation against opposing agents and new financial, cultural and political conditions.
%
%
%
%\section*{Exercise 2}
%
%\subsection*{Part A}
%
%The solution is given in Figure \ref{fig:exercise2}.
%
%\begin{figure}[htpb]
%\centering
%\makebox[\textwidth]{%
%\includegraphics[width=0.3\textwidth, angle =-90, trim = 25mm 25mm 100mm 25mm,clip=true]{images/exercise2.pdf}}
%\caption{State space for states 1 to 15, starting from 1, while the successor function returning two states, $2n$ and $2n+1$.}
%\label{fig:exercise2}
%% Place the label just after the caption to make the link work
%\end{figure} % table makes a floating object with a title
%
%\subsection*{Part B}
%
%The order in which the nodes will be visited for each type of search is given below:
%
%\begin{itemize}
%
%\item Breadth-First Search: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11
%
%\item Depth-Limited Search (depth limit 3): 1, 2, 4, 8, 9, 5, 10, 11
%
%\item Iterative Deepening Search: (depth limit 0) 1 - (depth limit 1) 1, 2, 3 - (depth limit 2) 1, 2, 4, 5, 3, 6, 7 - (depth limit 3) 1, 2, 4, 8, 9, 5, 10, 11
%
%\end{itemize}
%
%\subsection*{Part C}
%
%Bidirectional search would be quite appropriate for this problem, since we could easily employ the inverse of the successor function for searching in the opposite direction (from goal to start state): $\lfloor n/2 \rfloor$. For example, the bidirectional search algorithm would work as follows:
%
%\begin{itemize}
%
%\item Search from start: 1, 2, 3
%
%\item Search from goal: 11, 5, (2)
%
%\end{itemize}
%
%The algorithm would terminate when the search from goal reaches node 2, which would have already been reached by the search from start. The final solution would therefore be 1, 2, 5, 11.
%
%Simple search from goal to start would actually allow us to avoid searching altogether because each node has only a single predecessor. Hence, the solution could simply be found in $\log_2{n}$ steps.
%
%\section*{Exercise 3}
%
%The solution is given in Figure \ref{fig:exercise3}.
%
%\begin{sidewaysfigure}[htpb]
%\centering
%\makebox[\textwidth]{%
%\includegraphics[width=0.55\textwidth, angle =-90, trim = 25mm 25mm 50mm 25mm,clip=true]{images/exercise3.pdf}}
%\caption{Stages in an A* search from Lugoj to Bucharest. Nodes are labeled with $f = g+ h$ (black font), and with the selected order of expansion (red font). The sequence of states that are considered during each iteration consists of the current leaf nodes. The algorithm expands the leaf node with the lowest value of $f$, replacing it in the sequence with all its successors (to be considered for the following iteration). The optimal solution is depicted with red nodes and edges.}
%\label{fig:exercise3}
%% Place the label just after the caption to make the link work
%\end{sidewaysfigure} % table makes a floating object with a title
%
%
%\section*{Exercise 4}
%
%\subsection*{Part A}
%
%Since only one state ($k=1$) is stored in memory after each iteration, it is iteratively succeeded by its best neighboring state like in hill climbing. Hence, the local beam search algorithm with $k=1$ is a simple hill climbing algorithm.
%
%\subsection*{Part B}
%
%With temperature $T=0$, we can say that the probability $e^{\Delta E/T}=0$ when $\Delta E \le 0$, namely only better neighboring solutions are accepted. Therefore, simulated annealing with $T=0$ is a first choice hill climbing algorithm.
%
%\subsection*{Part C}
%
%If $N=1$, the population will consist of a single individual. Crossover will thus happen between (two copies of) that individual, resulting in the exact same solution. The random mutation mechanism will introduce a small number of point changes during each iteration, consequently turning genetic algorithm into a random walk.






